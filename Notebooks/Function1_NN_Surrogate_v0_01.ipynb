{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK57bCX8Pf1N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  Neural Network Surrogate for Black-Box Optimisation (BBO)\n",
        "# ============================================================\n",
        "#  Overview:\n",
        "#  This code trains a PyTorch neural network surrogate model to\n",
        "#  approximate a black-box function and identify the next best\n",
        "#  query point via Monte Carlo sampling.\n",
        "#\n",
        "#  Core idea:\n",
        "#  - The surrogate learns the relationship between inputs (X)\n",
        "#    and outputs (y).\n",
        "#  - Monte Carlo sampling generates many random candidates.\n",
        "#  - The surrogate predicts their outputs.\n",
        "#  - The best candidate (highest predicted y) is chosen as the\n",
        "#    next query point.\n",
        "#\n",
        "#  Monte Carlo approach:\n",
        "#  ---------------------\n",
        "#  Since black-box functions are unknown and costly to query,\n",
        "#  Monte Carlo sampling provides an efficient, probabilistic\n",
        "#  method to explore the input space and estimate promising\n",
        "#  regions for improvement.\n",
        "#\n",
        "#  Exploration vs. Exploitation Controls:\n",
        "#  --------------------------------------\n",
        "#  - Learning Rate (`lr`):\n",
        "#       ↑ Higher → faster, noisier updates (more exploration)\n",
        "#       ↓ Lower  → slower, stable convergence (more exploitation)\n",
        "#  - Batch Size (`batch_size`):\n",
        "#       ↓ Smaller → more stochastic gradient (exploration)\n",
        "#       ↑ Larger  → smoother gradient, focus on known good regions (exploitation)\n",
        "#  - Monte Carlo Samples (`n_candidates`):\n",
        "#       ↑ More → broader coverage of search space (exploration)\n",
        "#       ↓ Fewer → faster computation, focus on known regions (exploitation)\n",
        "#\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Set random seeds for reproducibility\n",
        "# ------------------------------------------------------------\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Define Neural Network Surrogate Model\n",
        "# ------------------------------------------------------------\n",
        "class SurrogateNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SurrogateNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Training Function with Optional Mini-Batching\n",
        "# ------------------------------------------------------------\n",
        "def train_surrogate(X_train, y_train, input_dim, lr=0.01, epochs=300, batch_size=None):\n",
        "    \"\"\"\n",
        "    Trains the neural network surrogate.\n",
        "    If batch_size is None, uses full-batch training.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SurrogateNN(input_dim).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    X_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "    if batch_size is not None:\n",
        "        dataset = TensorDataset(X_t, y_t)\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        loader = [(X_t, y_t)]  # full-batch fallback\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Monte Carlo Sampling to Find Next Query Point\n",
        "# ------------------------------------------------------------\n",
        "def suggest_next_query(model, input_dim, n_candidates=10000):\n",
        "    \"\"\"\n",
        "    Uses Monte Carlo sampling to suggest the next query point.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "\n",
        "    # Generate random candidate points uniformly in [0,1]^d\n",
        "    candidates = np.random.rand(n_candidates, input_dim)\n",
        "    with torch.no_grad():\n",
        "        preds = model(torch.tensor(candidates, dtype=torch.float32).to(device)).cpu().numpy()\n",
        "\n",
        "    # Select candidate with highest predicted value\n",
        "    best_idx = np.argmax(preds)\n",
        "    x_next = candidates[best_idx]\n",
        "\n",
        "    return x_next, preds[best_idx]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Denormalisation Helper Function\n",
        "# ------------------------------------------------------------\n",
        "def denormalize_results(x_next_norm, pred_y_norm):\n",
        "    \"\"\"\n",
        "    Convert the normalised next query and predicted output back\n",
        "    to their original scales.\n",
        "    \"\"\"\n",
        "    # Recover X\n",
        "    x_next_real = x_scaler.inverse_transform(x_next_norm.reshape(1, -1)).flatten()\n",
        "\n",
        "    # Recover y\n",
        "    y_pred_real = pred_y_norm * y_std + y_mean\n",
        "\n",
        "    return x_next_real, y_pred_real\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Example Usage\n",
        "# ------------------------------------------------------------\n",
        "# Assume you have:\n",
        "#   X: 2D numpy array of inputs (n_samples x n_features)\n",
        "#   y: 1D numpy array of corresponding outputs\n",
        "\n",
        "# Example placeholders (replace with real data)\n",
        "# X = np.load(\"initial_inputs.npy\")\n",
        "# y = np.load(\"initial_outputs.npy\")\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "function = '8'  ## Enter the function number being processed as a string.\n",
        "path = 'drive/MyDrive/data/' # Enter path to your initial files\n",
        "given_X = path + 'fn' + function + '_initial_inputs.npy'\n",
        "given_y = path + 'fn' + function + '_initial_outputs.npy'\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Load initial data\n",
        "# ------------------------------------------------------------\n",
        "X = np.load(given_X)        # shape (n0, d)\n",
        "y = np.load(given_y)       # shape (n0,)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Append new information\n",
        "# ------------------------------------------------------------\n",
        "if function == '1':\n",
        "  X = np.append(X,[[0.701438, 0.897524]], axis=0)  # Append week1 inputs\n",
        "  y = np.append(y, 4.907008577514035e-55)         # Append week1 outputs\n",
        "  X = np.append(X,[[0.608788, 0.208839]], axis=0)  # Append week2 inputs\n",
        "  y = np.append(y, -3.407789887257921e-57)         # Append week2 outputs\n",
        "  X = np.append(X,[[0.937387, 0.537998]], axis=0)  # Append week3 inputs\n",
        "  y = np.append(y, -1.8248983945966781e-72)         # Append week3 outputs\n",
        "  X = np.append(X,[[0.401144, 0.347316]], axis=0)  # Append week4 inputs\n",
        "  y = np.append(y, -0.000052943333535254514)         # Append week4 outputs\n",
        "elif function == '2':\n",
        "  X = np.append(X,[[0.695813, 0.000000]], axis=0)  # Append week inputs\n",
        "  y = np.append(y, 0.6257440183692108)         # Append week1 outputs\n",
        "  X = np.append(X,[[0.969061, 0.774664]], axis=0)  # Append week2 inputs\n",
        "  y = np.append(y, 0.033917749094575816)         # Append week2 outputs\n",
        "  X = np.append(X,[[0.690739, 0.962975]], axis=0)  # Append week3 inputs\n",
        "  y = np.append(y, 0.6440194651539941)         # Append week3 outputs\n",
        "  X = np.append(X,[[0.000000, 0.982469]], axis=0)  # Append week4 inputs\n",
        "  y = np.append(y, 0.011920662637457252)         # Append week4 outputs\n",
        "elif function == '3':\n",
        "  X = np.append(X,[[0.000000, 0.000000, 0.000000]], axis=0)  # Append week1 inputs\n",
        "  y = np.append(y, -0.1755204613669372)         # Append week1 outputs\n",
        "  X = np.append(X,[[0.999999, 0.183024, 0.788109]], axis=0)  # Append week2 inputs\n",
        "  y = np.append(y, -0.10594389054163186)         # Append week2 outputs\n",
        "  X = np.append(X,[[0.937840, 0.999999, 0.405928]], axis=0)  # Append week3 inputs\n",
        "  y = np.append(y, -0.05889151139553504)         # Append week3 outputs\n",
        "  X = np.append(X,[[0.523777, 0.465195, 0.442681]], axis=0)  # Append week4 inputs\n",
        "  y = np.append(y, -0.005996811424753983)         # Append week4 outputs\n",
        "elif function == '4':\n",
        "  X = np.append(X,[[0.440415, 0.425453, 0.378353, 0.397106]], axis=0)  # Append week1 inputs\n",
        "  y = np.append(y, 0.2601025838576061)         # Append week1 outputs\n",
        "  X = np.append(X,[[0.410011, 0.415255, 0.344454, 0.438808]], axis=0)  # Append week2 inputs\n",
        "  y = np.append(y, 0.3983591561199096)         # Append week2 outputs\n",
        "  X = np.append(X,[[0.384038, 0.412994, 0.399499, 0.436890]], axis=0)  # Append week3 inputs\n",
        "  y = np.append(y, 0.2987152140170406)         # Append week3 outputs\n",
        "  X = np.append(X,[[0.391529, 0.451226, 0.356835, 0.420242]], axis=0)  # Append week4 inputs\n",
        "  y = np.append(y, 0.06384840143364956)         # Append week4 outputs\n",
        "elif function == '5':\n",
        "  X = np.append(X,[[0.000000, 0.827185, 0.999999, 0.999999]], axis=0)  # Append week1 inputs\n",
        "  y = np.append(y, 2781.638812419282)         # Append week1 outputs\n",
        "  X = np.append(X,[[0.058940, 0.195873, 0.872581, 0.998561]], axis=0)  # Append week2 inputs\n",
        "  y = np.append(y, 848.8896402098709)         # Append week2 outputs\n",
        "  X = np.append(X,[[0.166206, 0.999999, 0.999999, 0.999999]], axis=0)  # Append week3 inputs\n",
        "  y = np.append(y, 4444.256290210307)         # Append week3 outputs\n",
        "  X = np.append(X,[[0.259704, 0.866240, 0.999999, 0.078304]], axis=0)  # Append week4 inputs\n",
        "  y = np.append(y, 837.6996715082641)         # Append week4 outputs\n",
        "elif function == '6':\n",
        "  X = np.append(X,[[0.464910, 0.242338, 0.574752, 0.999999, 0.000000]], axis=0)  # Append week1 inputs\n",
        "  y = np.append(y, -0.5265043497038704)         # Append week1 outputs\n",
        "  X = np.append(X,[[0.537065, 0.317653, 0.626041, 0.898937, 0.148218]], axis=0)  # Append week2 inputs\n",
        "  y = np.append(y, -0.41510731628352715)         # Append week2 outputs\n",
        "  X = np.append(X,[[0.455585, 0.174069, 0.999999, 0.999999, 0.292903]], axis=0)  # Append week3 inputs\n",
        "  y = np.append(y, -1.0104998624615082)         # Append week3 outputs\n",
        "  X = np.append(X,[[0.525055, 0.369255, 0.490228, 0.765387, 0.025091]], axis=0)  # Append week4 inputs\n",
        "  y = np.append(y, -0.3481623700173726)         # Append week4 outputs\n",
        "elif function == '7':\n",
        "  X = np.append(X,[[0.000000, 0.247036, 0.408965, 0.217149, 0.377534, 0.746590]], axis=0)  # Append week1 inputs\n",
        "  y = np.append(y, 2.3034568430941222)         # Append week1 outputs\n",
        "  X = np.append(X,[[0.000000, 0.181841, 0.435826, 0.062982, 0.361647, 0.858489]], axis=0)  # Append week2 inputs\n",
        "  y = np.append(y, 1.3279380639726712)         # Append week2 outputs\n",
        "  X = np.append(X,[[0.000000, 0.166738, 0.211207, 0.080984, 0.381370, 0.746608]], axis=0)  # Append week3 inputs\n",
        "  y = np.append(y, 1.433484749556216)         # Append week3 outputs\n",
        "  X = np.append(X,[[0.000000, 0.198493, 0.750365, 0.247059, 0.370826, 0.980737]], axis=0)  # Append week4 inputs\n",
        "  y = np.append(y, 1.239424289285777)         # Append week4 outputs\n",
        "elif function == '8':\n",
        "  X = np.append(X,[[0.060275, 0.000000, 0.134973, 0.000000, 0.999999, 0.404343, 0.057755, 0.516689]], axis=0)  # Append week1 inputs\n",
        "  y = np.append(y, 9.8814582425914)         # Append week1 outputs\n",
        "  X = np.append(X,[[0.122654, 0.153991, 0.162413, 0.045600, 0.999999, 0.536650, 0.260832, 0.932950]], axis=0)  # Append week2 inputs\n",
        "  y = np.append(y, 9.9450768395815)         # Append week2 outputs\n",
        "  X = np.append(X,[[0.085442, 0.318585, 0.000000, 0.239064, 0.999999, 0.927570, 0.142651, 0.932950]], axis=0)  # Append week3 inputs\n",
        "  y = np.append(y, 9.6920435401985)         # Append week3 outputs\n",
        "  X = np.append(X,[[0.266427, 0.000000, 0.185600, 0.000000, 0.999999, 0.210401, 0.179882, 0.156373]], axis=0)  # Append week4 inputs\n",
        "  y = np.append(y, 9.7659726871796)         # Append week4 outputs\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Normalise X and y\n",
        "# ------------------------------------------------------------\n",
        "# After loading your data (e.g. X, y = np.load(...))\n",
        "x_scaler = StandardScaler()\n",
        "Xn = x_scaler.fit_transform(X)\n",
        "\n",
        "y_mean = y.mean()\n",
        "y_std = y.std() if y.std() > 0 else 1.0\n",
        "yn = (y - y_mean) / y_std   # Normalize target manually (avoid sklearn overhead)\n",
        "\n",
        "\n",
        "input_dim = Xn.shape[1]\n",
        "batch_size = 8  # comment this line to use full-batch training\n",
        "\n",
        "# Train surrogate model\n",
        "model = train_surrogate(Xn, yn, input_dim=input_dim, lr=0.01, epochs=500, batch_size=batch_size)\n",
        "\n",
        "# Suggest next query point using Monte Carlo search\n",
        "x_next_norm, y_pred_norm = suggest_next_query(model, input_dim=input_dim, n_candidates=20000)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Convert back to original scales\n",
        "# ------------------------------------------------------------\n",
        "x_next, y_pred = denormalize_results(x_next_norm, y_pred_norm)\n",
        "\n",
        "print(\"Suggested next input (x_next):\", '-'.join(map(str, np.round(x_next, 6))))\n",
        "print(\"Predicted output estimate:\", float(y_pred[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlAehr_YPtO_",
        "outputId": "74c40d52-a743-4590-e7e4-9ef46c03d094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggested next input (x_next): 0.514029-0.459555-0.534367-0.406509-0.74451-0.66331-0.578111-0.756422\n",
            "Predicted output estimate: 8.434964868576083\n"
          ]
        }
      ]
    }
  ]
}