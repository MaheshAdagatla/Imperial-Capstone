{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  Neural Network Surrogate for Black-Box Optimisation (BBO)\n",
        "# ============================================================\n",
        "#  Overview:\n",
        "#  This code trains a PyTorch neural network surrogate model to\n",
        "#  approximate a black-box function and identify the next best\n",
        "#  query point via Monte Carlo sampling.\n",
        "#\n",
        "#  Core idea:\n",
        "#  - The surrogate learns the relationship between inputs (X)\n",
        "#    and outputs (y).\n",
        "#  - Monte Carlo sampling generates many random candidates.\n",
        "#  - The surrogate predicts their outputs.\n",
        "#  - The best candidate (highest predicted y) is chosen as the\n",
        "#    next query point.\n",
        "#\n",
        "#  Monte Carlo approach:\n",
        "#  ---------------------\n",
        "#  Since black-box functions are unknown and costly to query,\n",
        "#  Monte Carlo sampling provides an efficient, probabilistic\n",
        "#  method to explore the input space and estimate promising\n",
        "#  regions for improvement.\n",
        "#\n",
        "#  Exploration vs. Exploitation Controls:\n",
        "#  --------------------------------------\n",
        "#  - Learning Rate (lr):\n",
        "#       Higher = faster, noisier updates (more exploration)\n",
        "#       Lower  = slower, stable convergence (more exploitation)\n",
        "#  - Batch Size (batch_size):\n",
        "#       Smaller = more stochastic gradient (exploration)\n",
        "#       Larger  = smoother gradient, focus on known good regions (exploitation)\n",
        "#  - Monte Carlo Samples (n_candidates):\n",
        "#       More = broader coverage of search space (exploration)\n",
        "#       Fewer = faster computation, focus on known regions (exploitation)\n",
        "#\n",
        "# ============================================================\n",
        "#\n",
        "# v0.02 - Full rewrite using Pytorch with clear functions for easier understanding.\n",
        "# v0.03 - Updated to use GP to explore and shortlist a region, then use NN for\n",
        "#         exploitation within the shortlisted region.\n",
        "# v0.04 - Updated to down-weight low importance dimensions\n"
      ],
      "metadata": {
        "id": "-3rx1v8RLUdv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iK57bCX8Pf1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfeb9bfa-83ba-498b-fe61-7b591a886147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount drive for loading initial input and output files\n",
        "def mount_drive():\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "mount_drive()\n",
        "\n",
        "# Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the initial files\n",
        "def load_initial_files(function='1'):\n",
        "  path = 'drive/MyDrive/data/'\n",
        "  given_X = path + 'fn' + function + '_initial_inputs.npy'\n",
        "  given_y = path + 'fn' + function + '_initial_outputs.npy'\n",
        "  X = np.load(given_X)        # shape (n0, d)\n",
        "  y = np.load(given_y)       # shape (n0,)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "# Append new data from recent queries\n",
        "def append_new_data(X, y, function):\n",
        "  if function == '1':\n",
        "    X = np.append(X,[[0.701438, 0.897524]], axis=0)  # Append week1 inputs\n",
        "    y = np.append(y, 4.907008577514035e-55)         # Append week1 outputs\n",
        "    X = np.append(X,[[0.608788, 0.208839]], axis=0)  # Append week2 inputs\n",
        "    y = np.append(y, -3.407789887257921e-57)         # Append week2 outputs\n",
        "    X = np.append(X,[[0.937387, 0.537998]], axis=0)  # Append week3 inputs\n",
        "    y = np.append(y, -1.8248983945966781e-72)         # Append week3 outputs\n",
        "    X = np.append(X,[[0.401144, 0.347316]], axis=0)  # Append week4 inputs\n",
        "    y = np.append(y, -0.000052943333535254514)         # Append week4 outputs\n",
        "    X = np.append(X,[[0.815411, 0.802906]], axis=0)  # Append week5 inputs\n",
        "    y = np.append(y, 2.3120136577868056e-46)         # Append week5 outputs\n",
        "    X = np.append(X,[[0.147927, 0.999351]], axis=0)  # Append week6 inputs\n",
        "    y = np.append(y, -1.0556884679861184e-256)         # Append week6 outputs\n",
        "    X = np.append(X,[[0.961902, 0.541510]], axis=0)  # Append week7 inputs\n",
        "    y = np.append(y, -4.446243588517271e-83)         # Append week7 outputs\n",
        "  elif function == '2':\n",
        "    X = np.append(X,[[0.695813, 0.000000]], axis=0)  # Append week inputs\n",
        "    y = np.append(y, 0.6257440183692108)         # Append week1 outputs\n",
        "    X = np.append(X,[[0.969061, 0.774664]], axis=0)  # Append week2 inputs\n",
        "    y = np.append(y, 0.033917749094575816)         # Append week2 outputs\n",
        "    X = np.append(X,[[0.690739, 0.962975]], axis=0)  # Append week3 inputs\n",
        "    y = np.append(y, 0.6440194651539941)         # Append week3 outputs\n",
        "    X = np.append(X,[[0.000000, 0.982469]], axis=0)  # Append week4 inputs\n",
        "    y = np.append(y, 0.011920662637457252)         # Append week4 outputs\n",
        "    X = np.append(X,[[0.819992, 0.888300]], axis=0)  # Append week5 inputs\n",
        "    y = np.append(y, -0.05572035846713359)         # Append week5 outputs\n",
        "    X = np.append(X,[[0.576225, 0.995582]], axis=0)  # Append week6 inputs\n",
        "    y = np.append(y, -0.05917879343392843)         # Append week6 outputs\n",
        "    X = np.append(X,[[0.686326, 0.048467]], axis=0)  # Append week7 inputs\n",
        "    y = np.append(y, 0.5502098056853213)         # Append week7 outputs\n",
        "  elif function == '3':\n",
        "    X = np.append(X,[[0.000000, 0.000000, 0.000000]], axis=0)  # Append week1 inputs\n",
        "    y = np.append(y, -0.1755204613669372)         # Append week1 outputs\n",
        "    X = np.append(X,[[0.999999, 0.183024, 0.788109]], axis=0)  # Append week2 inputs\n",
        "    y = np.append(y, -0.10594389054163186)         # Append week2 outputs\n",
        "    X = np.append(X,[[0.937840, 0.999999, 0.405928]], axis=0)  # Append week3 inputs\n",
        "    y = np.append(y, -0.05889151139553504)         # Append week3 outputs\n",
        "    X = np.append(X,[[0.523777, 0.465195, 0.442681]], axis=0)  # Append week4 inputs\n",
        "    y = np.append(y, -0.005996811424753983)         # Append week4 outputs\n",
        "    X = np.append(X,[[0.503614, 0.496940, 0.415922]], axis=0)  # Append week5 inputs\n",
        "    y = np.append(y, -0.0062783618309617175)         # Append week5 outputs\n",
        "    X = np.append(X,[[0.306294, 0.000901, 0.737840]], axis=0)  # Append week6 inputs\n",
        "    y = np.append(y, -0.1868873865434488)         # Append week6 outputs\n",
        "    X = np.append(X,[[0.785819, 0.951069, 0.007398]], axis=0)  # Append week7 inputs\n",
        "    y = np.append(y, -0.12354269078229708)         # Append week7 outputs\n",
        "  elif function == '4':\n",
        "    X = np.append(X,[[0.440415, 0.425453, 0.378353, 0.397106]], axis=0)  # Append week1 inputs\n",
        "    y = np.append(y, 0.2601025838576061)         # Append week1 outputs\n",
        "    X = np.append(X,[[0.410011, 0.415255, 0.344454, 0.438808]], axis=0)  # Append week2 inputs\n",
        "    y = np.append(y, 0.3983591561199096)         # Append week2 outputs\n",
        "    X = np.append(X,[[0.384038, 0.412994, 0.399499, 0.436890]], axis=0)  # Append week3 inputs\n",
        "    y = np.append(y, 0.2987152140170406)         # Append week3 outputs\n",
        "    X = np.append(X,[[0.391529, 0.451226, 0.356835, 0.420242]], axis=0)  # Append week4 inputs\n",
        "    y = np.append(y, 0.06384840143364956)         # Append week4 outputs\n",
        "    X = np.append(X,[[0.529644, 0.477474, 0.456444, 0.497979]], axis=0)  # Append week5 inputs\n",
        "    y = np.append(y, -3.620742573150156)         # Append week5 outputs\n",
        "    X = np.append(X,[[0.362616, 0.390935, 0.358916, 0.408803]], axis=0)  # Append week6 inputs\n",
        "    y = np.append(y, 0.5941496830193782)         # Append week6 outputs\n",
        "    X = np.append(X,[[0.408168, 0.424670, 0.350333, 0.464759]], axis=0)  # Append week7 inputs\n",
        "    y = np.append(y, -0.28307671964285275)         # Append week7 outputs\n",
        "  elif function == '5':\n",
        "    X = np.append(X,[[0.000000, 0.827185, 0.999999, 0.999999]], axis=0)  # Append week1 inputs\n",
        "    y = np.append(y, 2781.638812419282)         # Append week1 outputs\n",
        "    X = np.append(X,[[0.058940, 0.195873, 0.872581, 0.998561]], axis=0)  # Append week2 inputs\n",
        "    y = np.append(y, 848.8896402098709)         # Append week2 outputs\n",
        "    X = np.append(X,[[0.166206, 0.999999, 0.999999, 0.999999]], axis=0)  # Append week3 inputs\n",
        "    y = np.append(y, 4444.256290210307)         # Append week3 outputs\n",
        "    X = np.append(X,[[0.259704, 0.866240, 0.999999, 0.078304]], axis=0)  # Append week4 inputs\n",
        "    y = np.append(y, 837.6996715082641)         # Append week4 outputs\n",
        "    X = np.append(X,[[0.405221, 0.818629, 0.840579, 0.874304]], axis=0)  # Append week5 inputs\n",
        "    y = np.append(y, 848.6842411794586)         # Append week5 outputs\n",
        "    X = np.append(X,[[0.379668, 0.980190, 0.997527, 0.939278]], axis=0)  # Append week6 inputs\n",
        "    y = np.append(y, 3506.2636242750236)         # Append week6 outputs\n",
        "    X = np.append(X,[[0.165658, 0.993001, 0.985802, 0.828885]], axis=0)  # Append week7 inputs\n",
        "    y = np.append(y, 2570.8680824924277)         # Append week7 outputs\n",
        "  elif function == '6':\n",
        "    X = np.append(X,[[0.464910, 0.242338, 0.574752, 0.999999, 0.000000]], axis=0)  # Append week1 inputs\n",
        "    y = np.append(y, -0.5265043497038704)         # Append week1 outputs\n",
        "    X = np.append(X,[[0.537065, 0.317653, 0.626041, 0.898937, 0.148218]], axis=0)  # Append week2 inputs\n",
        "    y = np.append(y, -0.41510731628352715)         # Append week2 outputs\n",
        "    X = np.append(X,[[0.455585, 0.174069, 0.999999, 0.999999, 0.292903]], axis=0)  # Append week3 inputs\n",
        "    y = np.append(y, -1.0104998624615082)         # Append week3 outputs\n",
        "    X = np.append(X,[[0.525055, 0.369255, 0.490228, 0.765387, 0.025091]], axis=0)  # Append week4 inputs\n",
        "    y = np.append(y, -0.3481623700173726)         # Append week4 outputs\n",
        "    X = np.append(X,[[0.541275, 0.517331, 0.640338, 0.739537, 0.379138]], axis=0)  # Append week5 inputs\n",
        "    y = np.append(y, -0.6094732138365111)         # Append week5 outputs\n",
        "    X = np.append(X,[[0.377302, 0.427069, 0.554688, 0.835249, 0.037887]], axis=0)  # Append week6 inputs\n",
        "    y = np.append(y, -0.3821980197564756)         # Append week6 outputs\n",
        "    X = np.append(X,[[0.399356, 0.472775, 0.272075, 0.930991, 0.004327]], axis=0)  # Append week7 inputs\n",
        "    y = np.append(y, -0.7153807487746384)         # Append week7 outputs\n",
        "  elif function == '7':\n",
        "    X = np.append(X,[[0.000000, 0.247036, 0.408965, 0.217149, 0.377534, 0.746590]], axis=0)  # Append week1 inputs\n",
        "    y = np.append(y, 2.3034568430941222)         # Append week1 outputs\n",
        "    X = np.append(X,[[0.000000, 0.181841, 0.435826, 0.062982, 0.361647, 0.858489]], axis=0)  # Append week2 inputs\n",
        "    y = np.append(y, 1.3279380639726712)         # Append week2 outputs\n",
        "    X = np.append(X,[[0.000000, 0.166738, 0.211207, 0.080984, 0.381370, 0.746608]], axis=0)  # Append week3 inputs\n",
        "    y = np.append(y, 1.433484749556216)         # Append week3 outputs\n",
        "    X = np.append(X,[[0.000000, 0.198493, 0.750365, 0.247059, 0.370826, 0.980737]], axis=0)  # Append week4 inputs\n",
        "    y = np.append(y, 1.239424289285777)         # Append week4 outputs\n",
        "    X = np.append(X,[[0.456843, 0.374177, 0.452545, 0.525118, 0.456228, 0.722847]], axis=0)  # Append week5 inputs\n",
        "    y = np.append(y, 0.8787401362260749)         # Append week5 outputs\n",
        "    X = np.append(X,[[0.002100, 0.111359, 0.320991, 0.376242, 0.229457, 0.955090]], axis=0)  # Append week6 inputs\n",
        "    y = np.append(y, 1.1302577537063068)         # Append week6 outputs\n",
        "    X = np.append(X,[[0.051607, 0.242133, 0.329607, 0.131528, 0.404377, 0.688463]], axis=0)  # Append week7 inputs\n",
        "    y = np.append(y, 1.974692744635984)         # Append week7 outputs\n",
        "  elif function == '8':\n",
        "    X = np.append(X,[[0.060275, 0.000000, 0.134973, 0.000000, 0.999999, 0.404343, 0.057755, 0.516689]], axis=0)  # Append week1 inputs\n",
        "    y = np.append(y, 9.8814582425914)         # Append week1 outputs\n",
        "    X = np.append(X,[[0.122654, 0.153991, 0.162413, 0.045600, 0.999999, 0.536650, 0.260832, 0.932950]], axis=0)  # Append week2 inputs\n",
        "    y = np.append(y, 9.9450768395815)         # Append week2 outputs\n",
        "    X = np.append(X,[[0.085442, 0.318585, 0.000000, 0.239064, 0.999999, 0.927570, 0.142651, 0.932950]], axis=0)  # Append week3 inputs\n",
        "    y = np.append(y, 9.6920435401985)         # Append week3 outputs\n",
        "    X = np.append(X,[[0.266427, 0.000000, 0.185600, 0.000000, 0.999999, 0.210401, 0.179882, 0.156373]], axis=0)  # Append week4 inputs\n",
        "    y = np.append(y, 9.7659726871796)         # Append week4 outputs\n",
        "    X = np.append(X,[[0.514029, 0.459555, 0.534367, 0.406509, 0.744510, 0.663310, 0.578111, 0.756422]], axis=0)  # Append week5 inputs\n",
        "    y = np.append(y, 8.6884084301446)         # Append week5 outputs\n",
        "    X = np.append(X,[[0.026980, 0.613170, 0.025877, 0.359845, 0.896836, 0.008212, 0.028519, 0.131032]], axis=0)  # Append week6 inputs\n",
        "    y = np.append(y, 9.3709013812716)         # Append week6 outputs\n",
        "    X = np.append(X,[[0.065492, 0.074877, 0.151838, 0.036631, 0.882332, 0.837480, 0.404222, 0.901998]], axis=0)  # Append week7 inputs\n",
        "    y = np.append(y, 9.7678761465696)         # Append week7 outputs\n",
        "\n",
        "  return X, y\n",
        "\n",
        "# Normalise X and y\n",
        "def normalise_data(X, y):\n",
        "  x_scaler = StandardScaler()\n",
        "  Xn = x_scaler.fit_transform(X)\n",
        "  y_mean = y.mean()\n",
        "  y_std = y.std() if y.std() > 0 else 1.0\n",
        "  yn = (y - y_mean) / y_std   # Normalize target manually (avoid sklearn overhead)\n",
        "\n",
        "  return Xn, yn, y_mean, y_std, x_scaler"
      ],
      "metadata": {
        "id": "D78VjqxB7Zkf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Neural Network surrogate model\n",
        "class SurrogateNN(nn.Module):\n",
        "    def __init__(self, input_dim, dropout_rate=0.2):\n",
        "        super(SurrogateNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Train Surrogate NN model with optional mini batching\n",
        "def train_nn(X_train, y_train, input_dim, lr=0.01, epochs=300, batch_size=None):\n",
        "    \"\"\"\n",
        "    Trains the neural network surrogate.\n",
        "    If batch_size is None, uses full-batch training.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    nn_model = SurrogateNN(input_dim).to(device)\n",
        "    optimizer = optim.Adam(nn_model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    X_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "    if batch_size is not None:\n",
        "        dataset = TensorDataset(X_t, y_t)\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        loader = [(X_t, y_t)]  # full-batch fallback\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = nn_model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return nn_model\n",
        "\n",
        "# Monte Carlo Sampling to Find Next Query Point\n",
        "def suggest_next_query(nn_model, input_dim, x_scaler, y_mean, y_std, n_candidates=10000):\n",
        "    \"\"\"\n",
        "    Uses Monte Carlo sampling to suggest the next query point.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    nn_model.eval()\n",
        "\n",
        "    # Generate random candidate points uniformly in [0,1]^d\n",
        "    candidates = np.random.rand(n_candidates, input_dim)\n",
        "\n",
        "    #Normalise candidates using fitted scaler\n",
        "    candidates_norm = x_scaler.transform(candidates)\n",
        "\n",
        "    # Predict for normalised candidates\n",
        "    with torch.no_grad():\n",
        "        preds_norm = model(torch.tensor(candidates_norm, dtype=torch.float32).to(device)).cpu().numpy()\n",
        "\n",
        "    # Select candidate with highest predicted value\n",
        "    best_idx = np.argmax(preds_norm)\n",
        "    x_next_norm = candidates_norm[best_idx]\n",
        "    y_pred_norm = preds_norm[best_idx]\n",
        "\n",
        "    # Denormalise back to original scale\n",
        "    x_next = x_scaler.inverse_transform(x_next_norm.reshape(1, -1)).flatten()\n",
        "    y_pred = y_pred_norm * y_std + y_mean\n",
        "\n",
        "    return x_next, y_pred"
      ],
      "metadata": {
        "id": "zqerH2bMFSoJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a surrogate GP model\n",
        "def train_gp(Xn, yn, kernel=None, n_restarts_optimizer=10, random_state=42):\n",
        "  if kernel is None:\n",
        "    d = Xn.shape[1]\n",
        "    kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(d),\n",
        "                                          length_scale_bounds=(1e-6, 1e3),\n",
        "                                          nu=2.5)\n",
        "    kernel += WhiteKernel(noise_level=1e-6, noise_level_bounds=(1e-8, 1e1))\n",
        "\n",
        "  gp_model = GaussianProcessRegressor(kernel=kernel,\n",
        "                                      normalize_y=False,\n",
        "                                      n_restarts_optimizer=n_restarts_optimizer,\n",
        "                                      random_state=random_state)\n",
        "  gp_model.fit(Xn, yn)\n",
        "\n",
        "  return gp_model\n",
        "\n",
        "# Use NN to predict the shortlisted batch (returns normalised preds)\n",
        "def nn_predict(nn_model, X_scaled, batch=1024):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  nn_model.to(device)\n",
        "  nn_model.eval()\n",
        "  nn_preds = []\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, X_scaled.shape[0], batch):\n",
        "      xb = X_scaled[i:i+batch]\n",
        "      xb_t = torch.tensor(xb, dtype=torch.float32).to(device)\n",
        "      preds = nn_model(xb_t).cpu().numpy().reshape(-1)\n",
        "      nn_preds.append(preds)\n",
        "\n",
        "  nn_preds = np.concatenate(nn_preds, axis=0)\n",
        "  return nn_preds\n",
        "\n",
        "# Two stage approach using GP and NN to find Next Query Point\n",
        "def suggest_next_query_two_stage(nn_model, gp_model, input_dim, x_scaler, y_mean, y_std,\n",
        "                                 stage1_size=20000, stage2_size=1000, gp_acq_k=1.96, nn_batch=1024):\n",
        "  \"\"\"\n",
        "    Two-stage selection:\n",
        "      1) Sample stage1_size candidates uniformly in [0,1]^d.\n",
        "      2) Scale them and evaluate GP (mu, std) -> acquisition = mu + gp_acq_k * std.\n",
        "      3) Shortlist top `shortlist` candidates by GP acquisition.\n",
        "      4) Re-rank shortlist by NN predicted mean (NN trained on scaled inputs).\n",
        "      5) Return raw chosen x (in [0,1]^d) and denormalised predicted y.\n",
        "\n",
        "    Returns:\n",
        "      x_next_raw: (d,) raw candidate in [0,1]\n",
        "      y_pred_raw: float predicted output in original scale\n",
        "  \"\"\"\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Generate random candidate points uniformly in [0,1]^d\n",
        "  #candidates = np.random.rand(stage1_size, input_dim)\n",
        "\n",
        "  # Generate random canditates based on dimension importance\n",
        "  gp_imp = get_gp_importance(gp_model)\n",
        "  nn_imp = get_nn_importance(nn_model, Xn)\n",
        "  importance = combine_importance(gp_imp, nn_imp)\n",
        "  candidates = sample_candidates_weighted(importance, n_samples=stage1_size)\n",
        "\n",
        "\n",
        "  #Normalise candidates using fitted scaler\n",
        "  candidates_norm = x_scaler.transform(candidates)\n",
        "\n",
        "  # GP predict on scaled candidates (returns normalized outputs)\n",
        "  mu_gp, sigma_gp = gp_model.predict(candidates_norm, return_std=True)     # both shape (stage1_size,)\n",
        "  # Acquisition: simple UCB-like\n",
        "  gp_acq = mu_gp + gp_acq_k * sigma_gp\n",
        "\n",
        "  # Shortlist top-k candidates by GP acquisition\n",
        "  top_idx = np.argsort(gp_acq)[-stage2_size:]\n",
        "  X_short_raw = candidates[top_idx]         # raw coordinates (shortlist)\n",
        "  X_short_scaled = candidates_norm[top_idx]   # scaled coordinates for NN\n",
        "\n",
        "  nn_preds_norm = nn_predict(nn_model, X_short_scaled, batch=nn_batch)\n",
        "  # nn_preds_norm are in normalized y-space (because NN trained on yn).\n",
        "  # We'll rank by normalized preds (same order as raw).\n",
        "  best_local = np.argmax(nn_preds_norm)\n",
        "  x_next = X_short_raw[best_local]            # keep in [0,1] space\n",
        "\n",
        "  # The NN predicted normalized y so we denormalize it\n",
        "  y_pred_norm = nn_preds_norm[best_local]\n",
        "  y_pred = float(y_pred_norm * y_std + y_mean)\n",
        "\n",
        "  return x_next, y_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "7YMjp2qQiGQ2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach to down-weight low importance dimensions\n",
        "# Get nn importance\n",
        "def get_nn_importance(nn_model, X_scaled):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  nn_model.to(device)\n",
        "  nn_model.eval()\n",
        "  X_t = torch.tensor(X_scaled, dtype=torch.float32, requires_grad=True)\n",
        "  preds = nn_model(X_t).sum()\n",
        "  preds.backward()\n",
        "  grads = X_t.grad.abs().mean(dim=0).numpy()\n",
        "  return grads / grads.sum()\n",
        "\n",
        "# Get gp importance\n",
        "def get_gp_importance(gp_model):\n",
        "  ls = np.array(gp_model.kernel_.k1.k2.length_scale).flatten()\n",
        "  raw = 1.0 / (ls + 1e-8)\n",
        "  return raw / raw.sum()\n",
        "\n",
        "# Commbine nn_importance and gp_importance\n",
        "def combine_importance(gp_imp, nn_imp, gp_weight=0.6):\n",
        "  imp = gp_weight * gp_imp + (1 - gp_weight) * nn_imp\n",
        "  return imp / imp.sum()\n",
        "\n",
        "# Generate candidates based on combined importance\n",
        "def sample_candidates_weighted(importance, n_samples=10000):\n",
        "    \"\"\"\n",
        "    Generate candidate samples where each dimension is sampled\n",
        "    more narrowly if its importance is low.\n",
        "\n",
        "    importance: array of shape [d] summing to 1\n",
        "    \"\"\"\n",
        "    dim = len(importance)\n",
        "\n",
        "    # Scale ranges: important dims sampled wide, weak dims sampled tight.\n",
        "    # Values stay within [0,1].\n",
        "    #\n",
        "    # scale = 1.0 => explore full [0,1]\n",
        "    # scale = 0.2 => explore only central 20%\n",
        "    #\n",
        "    scale = 0.1 + 0.9 * importance  # ensures minimum variety\n",
        "\n",
        "    # Sample raw uniform values\n",
        "    raw = np.random.rand(n_samples, dim)\n",
        "\n",
        "    # Apply scaling around 0.5 (centralised exploration in weak dims)\n",
        "    candidates = 0.5 + (raw - 0.5) * scale\n",
        "\n",
        "    # Clip to ensure valid bounds\n",
        "    candidates = np.clip(candidates, 0.0, 1.0)\n",
        "\n",
        "    return candidates"
      ],
      "metadata": {
        "id": "P_v3EaHYIOsw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide function and batch_size\n",
        "func = '8'       # number between 1 and 9 representing BBO function\n",
        "batch_size = 8   # number or None\n",
        "\n",
        "# Load data and normalise\n",
        "X, y = load_initial_files(function=func)\n",
        "X, y = append_new_data(X, y, function=func)\n",
        "Xn, yn, y_mean, y_std, x_scaler = normalise_data(X, y)\n",
        "\n",
        "print(len(Xn))\n",
        "print(max(y))\n",
        "print(max(yn))\n",
        "\n",
        "# Calcualte input shape\n",
        "input_dim = Xn.shape[1]\n",
        "\n",
        "# Train NN model\n",
        "nn_model = train_nn(Xn, yn, input_dim, lr=0.01, epochs=500, batch_size=batch_size)\n",
        "\n",
        "# Train GP model\n",
        "gp_model = train_gp(Xn, yn, kernel=None, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "# Get the Next Query Point\n",
        "#x_next, y_pred = suggest_next_query(model, input_dim, x_scaler, y_mean, y_std, n_candidates=50000)\n",
        "x_next, y_pred = suggest_next_query_two_stage(nn_model, gp_model, input_dim, x_scaler, y_mean, y_std,\n",
        "  stage1_size=20000, stage2_size=1000, gp_acq_k=1.96, nn_batch=1024\n",
        ")\n",
        "\n",
        "print(\"\\nFunction:\", func)\n",
        "print(\"\\nSuggested next input (x_next):\", '-'.join(map(str, np.round(x_next, 6))))\n",
        "#print(\"\\nPredicted output estimate:\", float(y_pred[0]))\n",
        "print(\"\\nPredicted output estimate:\", float(y_pred))\n"
      ],
      "metadata": {
        "id": "sY5CPq-z8xvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db981a3-e367-485d-e971-a900a6cc06df"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "9.9450768395815\n",
            "1.7136810066874795\n",
            "\n",
            "Function: 8\n",
            "\n",
            "Suggested next input (x_next): 0.416048-0.59922-0.417048-0.424239-0.551884-0.585992-0.386618-0.514817\n",
            "\n",
            "Predicted output estimate: 9.054074978896033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-08. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Week 8\n",
        "Function: 1\n",
        "\n",
        "Suggested next input (x_next): 0.354466-0.463725\n",
        "\n",
        "Predicted output estimate: -6.573937079029337e-05\n",
        "\n",
        "Function: 2\n",
        "\n",
        "Suggested next input (x_next): 0.684793-0.736977\n",
        "\n",
        "Predicted output estimate: 0.5478891261102885\n",
        "\n",
        "Function: 3\n",
        "\n",
        "Suggested next input (x_next): 0.671012-0.54088-0.410996\n",
        "\n",
        "Predicted output estimate: -0.07154796838749342\n",
        "\n",
        "Function: 4\n",
        "\n",
        "Suggested next input (x_next): 0.462778-0.459808-0.427924-0.383695\n",
        "\n",
        "Predicted output estimate: 1.4924733828472245\n",
        "\n",
        "Function: 5\n",
        "\n",
        "Suggested next input (x_next): 0.450158-0.310399-0.558612-0.719998\n",
        "\n",
        "Predicted output estimate: 416.6177720858795\n",
        "\n",
        "Function: 6\n",
        "\n",
        "Suggested next input (x_next): 0.352618-0.580837-0.570192-0.621771-0.372965\n",
        "\n",
        "Predicted output estimate: -0.5151560075399452\n",
        "\n",
        "Function: 7\n",
        "\n",
        "Suggested next input (x_next): 0.391166-0.399851-0.48006-0.389445-0.406096-0.616639\n",
        "\n",
        "Predicted output estimate: 1.7499281039217427\n",
        "\n",
        "Function: 8\n",
        "\n",
        "Suggested next input (x_next): 0.416048-0.59922-0.417048-0.424239-0.551884-0.585992-0.386618-0.514817\n",
        "\n",
        "Predicted output estimate: 9.054074978896033\n",
        "\n"
      ],
      "metadata": {
        "id": "fF8gtzSbTE6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **## Week7**\n",
        "Function: 1\n",
        "\n",
        "Suggested next input (x_next): 0.961902-0.54151\n",
        "\n",
        "Predicted output estimate: 0.0003957174409107497\n",
        "\n",
        "Function: 2\n",
        "\n",
        "Suggested next input (x_next): 0.686326-0.048467\n",
        "\n",
        "Predicted output estimate: 0.5931130065069816\n",
        "\n",
        "Function: 3\n",
        "\n",
        "Suggested next input (x_next): 0.785819-0.951069-0.007398\n",
        "\n",
        "Predicted output estimate: -0.07821291139128447\n",
        "\n",
        "Function: 4\n",
        "\n",
        "Suggested next input (x_next): 0.408168-0.42467-0.350333-0.464759\n",
        "\n",
        "Predicted output estimate: 0.3892309433321408\n",
        "\n",
        "Function: 5\n",
        "\n",
        "Suggested next input (x_next): 0.165658-0.993001-0.985802-0.828885\n",
        "\n",
        "Predicted output estimate: 3679.3006173709423\n",
        "\n",
        "Function: 6\n",
        "\n",
        "Suggested next input (x_next): 0.399356-0.472775-0.272075-0.930991-0.004327\n",
        "\n",
        "Predicted output estimate: -0.33820016205812065\n",
        "\n",
        "Function: 7\n",
        "\n",
        "Suggested next input (x_next): 0.051607-0.242133-0.329607-0.131528-0.404377-0.688463\n",
        "\n",
        "Predicted output estimate: 1.993543925493233\n",
        "\n",
        "Function: 8\n",
        "\n",
        "Suggested next input (x_next): 0.065492-0.074877-0.151838-0.036631-0.882332-0.83748-0.404222-0.901998\n",
        "\n",
        "Predicted output estimate: 9.877420081086175\n"
      ],
      "metadata": {
        "id": "cCDAEDEfM9ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **## Week6**\n",
        "\n",
        "Function: 1\n",
        "\n",
        "Suggested next input (x_next): 0.147927-0.999351\n",
        "\n",
        "Predicted output estimate: 0.004738142161169105\n",
        "\n",
        "Function: 2\n",
        "\n",
        "Suggested next input (x_next): 0.576225-0.995582\n",
        "\n",
        "Predicted output estimate: 1.0778257373391364\n",
        "\n",
        "Function: 3\n",
        "\n",
        "Suggested next input (x_next): 0.306294-0.000901-0.737840\n",
        "\n",
        "Predicted output estimate: 0.03597608387388612\n",
        "\n",
        "Function: 4\n",
        "\n",
        "Suggested next input (x_next): 0.362616-0.390935-0.358916-0.408803\n",
        "\n",
        "Predicted output estimate: 0.4543251119036995\n",
        "\n",
        "Function: 5\n",
        "\n",
        "Suggested next input (x_next): 0.379668-0.980190-0.997527-0.939278\n",
        "\n",
        "Predicted output estimate: 4366.357278991082\n",
        "\n",
        "Function: 6\n",
        "\n",
        "Suggested next input (x_next): 0.377302-0.427069-0.554688-0.835249-0.037887\n",
        "\n",
        "Predicted output estimate: -0.23066364279575358\n",
        "\n",
        "Function: 7\n",
        "\n",
        "Suggested next input (x_next): 0.002100-0.111359-0.320991-0.376242-0.229457-0.955090\n",
        "\n",
        "Predicted output estimate: 2.464740596873976\n",
        "\n",
        "Function: 8\n",
        "\n",
        "Suggested next input (x_next): 0.026980-0.613170-0.025877-0.359845-0.896836-0.008212-0.028519-0.131032\n",
        "\n",
        "Predicted output estimate: 10.435474844266569\n",
        "\n"
      ],
      "metadata": {
        "id": "Mn3uM-cp3H4a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_Ueln1kSCIt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}